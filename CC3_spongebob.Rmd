---
title: "R Notebook"
output: 
  github_document:
    toc: true
    toc_depth: 2
---
# Dada2
## chargement des librairies
```{r}
library(dada2)
library(phyloseq)
library(ggplot2)
library(dplyr)
library(reshape2)
library(ade4)
library(ggrepel)
library(lattice)
library(caret)
library(igraph)
library(ggnetwork)
theme_set(theme_bw())
```
Ici nous récupérons la taxonomie silva afin d'analyser et d'assigner les taxonomies.
```{bash}
wget https://zenodo.org/record/3986799/files/silva_nr99_v138_train_set.fa.gz
wget https://zenodo.org/record/3986799/files/silva_species_assignment_v138.fa.gz
```

```{r}
sponge_metadata <- read.delim("SraRunTable.txt", header=TRUE, sep=",")
```


```{r}
path <- "~/CC3_sponge/Samples" # CHANGE ME to the directory containing the fastq files after unzipping.
list.files(path)
```
## filtration et éliminations des sequences basse qualité
```{r}
# Sort ensures forward/reverse reads are in same order
fnFs <- sort(list.files(path, pattern="_1.fastq.gz"))
fnRs <- sort(list.files(path, pattern="_2.fastq.gz"))
# Extract sample names, assuming filenames have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`,1)
# Specify the full path to the fnFs and fnRs
fnFs <- file.path(path, fnFs)
fnRs <- file.path(path, fnRs)
print(fnRs)
```
Le plot quality profile nous permets d'analyser les nucléotides pour lesquels la qualité du séquençage décroit. 
Pour les séquences forward, qui sont de meilleures qulités que les reverses dû à la méthode de séquençage, la qualité décroit à partir de 240 nucléotides. 
```{r}
plotQualityProfile(fnFs[1:3])
```

```{r}
plotQualityProfile(fnRs[1:3])
```

```{r}
# Place filtered files in filtered/ subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names
sample.names
print(filtFs)
```
Nous coupons ici les séquences au niveau des nucléotides précédemment énoncés. Nous choisissons de ne couper qu'à 200 nucléotides pour les reverse pour garder un overlap assez important. 
```{r}
out<-filterAndTrim(fnFs, filtFs, fnRs, filtRs, truncLen=c(280,220),
              maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=TRUE) # On Windows set multithread=FALSE
```

```{r}
head(out)
```
# Learn the Error Rates
 Nous allons ici utiliser des lignes de commandes qui vont permettre d'apprendre à la machine les différents profils d'erreurs générées lors du séquençage. L'opération est faite sur les séquences reverse et forward.
 
```{r}
errF <- learnErrors(filtFs, multithread=TRUE)
```

```{r}
errR <- learnErrors(filtRs, multithread=TRUE)
```
ici nous visualisons la probabilité d'obtenir une erreur de la machine remplaçant une base par une autre (A→C, A→G, ...) le taux d'erreur sont indiqués pour chaque combinaison possible pour les séquences reverse et forward. 
Chaque point représentent les taux d'erreur observés pour chaque score de qualité du consensus. La ligne noire montre le taux d'erreur estimés après convergence de l'algorithme d'apprentissage machine et la ligne rouge montre le taux d'erreur attendus selon la définition nominale du Q-score.
```{r}
plotErrors(errF, nominalQ=TRUE)
```

```{r}
plotErrors(errR, nominalQ=TRUE)
```
# Sample Inference
Ici nous créons une autre variable "dadaFs" dans laquelle nous mettons les fichiers obtenus après avoir filtré et appliqué le profil d'erreur à nos séquences. Nous allons faire la même chose avec dadaRS.

```{r}
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
```

```{r}
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```
Cette commande nous permet de visualiser le résultat global qu'on retrouve classé dans la liste dadaFs. Ils nous indiquent que sur les séquences on retrouve 1010 séquences qui correspondent aux vrais variants, par rapport aux 37907 séquences. 
```{r}
dadaFs[[1]]
```
# Merge paired reads
Ici nous voulons mettre en une seule séquence double brin les Forwards et les Reverses. Nous pouvons faire cette opération grâce aux overlaps. Cela se fait grâce à un alignement entre les forwards et les reverses qui vont permettre de contruire les contigs.
```{r}
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```
# Construct sequence table
Nous allons construire une table des variations de séquence dans les amplicons (ASV) qui permet une meilleure résolution que les tables OTUs 97%
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
```
```{r}
# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```
# Remove chimeras
Malgré qu'on ait pu appliquer les modèles d'erreurs aux séquences, il reste des chimères. Ces chimères sont facilement reconnaissables par la machine et peuvent etre réparées en y rajoutant les parties droites et gauche des 2 séquences les plus abondantes.
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)
```
Ici on peut voir qu'on à 22% de chimères dans notre jeu de donnée. Ce chiffre important peut être dû à la qualité des séquences reverse qui était assez moyenne sur la fin.
```{r}
1-sum(seqtab.nochim)/sum(seqtab)
```
# Track reads through the pipeline
Ce code nous permet de visualiser le nombre de séquences obtenues à la suite de toutes nos manipulations de filtrage. Ici nous pouvons voir qu'on a pu récupérer la plupart de nos séquences brutes, ce qui est signe d'une bonne qualité de séquençage globale (malgré les 22% de chimères).
```{r}
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
# Assign taxonomy

Nous créons ainsi une variable qui va recevoir les espèces obtenues grâce à Silva

```{r}
taxa <- assignTaxonomy(seqtab.nochim, "/home/rstudio/CC3_sponge/silva_nr99_v138_train_set.fa.gz", multithread=TRUE)
```

```{r}
taxa <- addSpecies(taxa, "/home/rstudio/CC3_sponge/silva_species_assignment_v138.fa.gz")
```
On remarque donc après avoir affiché la table qu'on a créée on obtient une majorité de  Alphaproteobacteries et plus précisément les bactéries de la clade SAR11 (=Candidatus Pelagibacter ubique). En effet, cet ordre de bactérie est l'ordre le plus représenté dans les océans : elle a une répartition mondiale. Ce résultat est donc cohérent. 
```{r}
taxa.print <- taxa # Removing sequence rownames for display only
rownames(taxa.print) <- NULL
head(taxa.print)
```
# création de l'arbre phyloseq 
```{r}
library(phangorn)
library(DECIPHER)
seqs <- getSequences(seqtab.nochim)
names(seqs) <- seqs # This propagates to the tip labels of the tree
alignment <- AlignSeqs(DNAStringSet(seqs), anchor=NA,verbose=FALSE)
phangAlign <- phyDat(as(alignment, "matrix"), type="DNA")
dm <- dist.ml(phangAlign)
treeNJ <- NJ(dm) # Note, tip order != sequence order
fit = pml(treeNJ, data=phangAlign)
fitGTR <- update(fit, k=4, inv=0.2)
fitGTR <- optim.pml(fitGTR, model="GTR", optInv=TRUE, optGamma=TRUE,
        rearrangement = "stochastic", control = pml.control(trace = 0))
detach("package:phangorn", unload=TRUE)
```
```{r}
ps <- phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE), 
               sample_data(samdf), 
               tax_table(taxa), phy_tree(fitGTR$tree))
ps
```
# création tableau 

```{r}
samples.out <- rownames(seqtab.nochim)
```

```{r}
subject <- samples.out
```

```{r}
subject2 <- substr(subject,4,5)
print(subject2)
```

```{r}
espece <- substr(subject,0,2)
print(espece)
```


```{r}
samdf <- data.frame(Subject=subject, Number=subject2, Espece=espece)
samdf$time[samdf$Number=="06"] <- "168h"
samdf$time[samdf$Number=="03"] <- "168h"
samdf$time[samdf$Number=="01"] <- "168h"
samdf$time[samdf$Number=="02"] <- "168h"
samdf$time[samdf$Number=="04"] <- "168h"
samdf$time[samdf$Number=="05"] <- "168h"
samdf$time[samdf$Number=="07"] <- "24h"
samdf$time[samdf$Number=="08"] <- "24h"
samdf$time[samdf$Number=="09"] <- "24h"
samdf$time[samdf$Number=="10"] <- "24h"
samdf$time[samdf$Number=="11"] <- "24h"
samdf$time[samdf$Number=="16"] <- "24h"
samdf$time[samdf$Number=="12"] <- "avant"
samdf$time[samdf$Number=="13"] <- "avant"
samdf$time[samdf$Number=="14"] <- "avant"
samdf$time[samdf$Number=="15"] <- "avant"
samdf$time[samdf$Number=="17"] <- "avant"
samdf$time[samdf$Number=="18"] <- "avant"
samdf$traitement[samdf$Number=="18"] <- "pulse"
samdf$traitement[samdf$Number=="06"] <- "pulse"
samdf$traitement[samdf$Number=="03"] <- "pulse"
samdf$traitement[samdf$Number=="01"] <- "pulse"
samdf$traitement[samdf$Number=="11"] <- "pulse"
samdf$traitement[samdf$Number=="09"] <- "pulse"
samdf$traitement[samdf$Number=="07"] <- "pulse"
samdf$traitement[samdf$Number=="17"] <- "pulse"
samdf$traitement[samdf$Number=="12"] <- "pulse"
samdf$traitement[samdf$Number=="05"] <- "controle"
samdf$traitement[samdf$Number=="04"] <- "controle"
samdf$traitement[samdf$Number=="02"] <- "controle"
samdf$traitement[samdf$Number=="16"] <- "controle"
samdf$traitement[samdf$Number=="10"] <- "controle"
samdf$traitement[samdf$Number=="08"] <- "controle"
samdf$traitement[samdf$Number=="15"] <- "controle"
samdf$traitement[samdf$Number=="13"] <- "controle"
samdf$traitement[samdf$Number=="14"] <- "controle"
samdf$Eponge[samdf$Espece=="CY"] <- "C. concentrica"
samdf$Eponge[samdf$Espece=="AQ"] <- "A. queenslandica"
samdf$Eponge[samdf$Espece=="CO"] <- "C. mathewsi"
samdf$Eponge[samdf$Espece=="IB"] <- "I. basta"
samdf$Eponge[samdf$Espece=="IR"] <- "I. ramosa"
samdf$Eponge[samdf$Espece=="ST"] <- "S. flabelliformis"
rownames(samdf) <-samples.out
```

```{r}
ps <-phyloseq(otu_table(seqtab.nochim, taxa_are_rows=FALSE),
sample_data(samdf),
tax_table(taxa))
ps <-prune_samples(sample_names(ps) != "Mock", ps) # Remove mock sample
```

```{r}
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps
```

```{r}
plot_richness(ps, x="time", measures=c("Shannon", "Simpson"), color="traitement")
```

```{r}
plot_richness(ps, x="sample.names", measures=c("Shannon"), color="Eponge", shape = "traitement")
```
## Filtrage de la taxonomie

```{r}
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
```
Ici nous allons mesurer la prévalence, qui sera dans le cadre de cette étude le nombre d'echantillons par taxon

```{r}
# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps),
                    tax_table(ps))
```
Cette commande nous permet d'evaluer la prévalence moyenne de chaque phylum (colonne1) et la prévalence totale (colonne2). Cela nous permet de confirmer les résultats du dessus et de pouvoir éliminer les phylums peu importants.
```{r}
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```
Le code permettant de retirer ces taxons est ci-dessous.
```{r}
# Define phyla to filter
filterPhyla = c("Campilobacterota", "Dependentiae", " Crenarchaeota","Desulfobacterota", "Dadabacteria ", "Fibrobacterota", " Hydrogenedentes", "NB1-j", " PAUC34f", " Elusimicrobiota", "Gemmatimonadota", "Myxococcota")
# Filter entries with unidentified Phylum.
ps1 = subset_taxa(ps, !Phylum %in% filterPhyla)
ps1
```
# Prevalence Filtering
 Ces manipulations nous permettent de voir si nous avons manqué de voir des echantillons mal definis ou en tres faible quantité qui devraient etre retirés. On va aussi pouvoir avoir un aperçu des séquences qui sont rangées dans chaque features. 
Ici; chaque point représente un taxa. Nous ne voyons pas de seuil de prévalence clairement établi ici. Nous avons donc des taxons assez stables. Néanmoins nous pouvons fixer manuelle le seuil de prévalence quelque part entre 0 et 10% (en verifiant qu'il n'y a pas d'impact non attendu sur la suite de l'étude)

Sans surprise, les phylums les plus représentés sont ceux qui ont le plus de prévalence. 
```{r}
# Subset to the remaining phyla
prevdf1 = subset(prevdf, Phylum %in% get_taxa_unique(ps1, "Phylum"))
ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps),color=Phylum)) +
  # Include a guess for parameter
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```
on va donc fixer un seuil de prévalence de 5%, c'est-à- dire que nous allons retirer toutes les valeurs de prévalence inferieures à 95%.
```{r}
# Define prevalence threshold as 5% of total samples
prevalenceThreshold = 0.05 * nsamples(ps)
prevalenceThreshold
```
C'est grâce à la fonction prune_taxa qu'on va pouvoir retirer les ASVs qui ne respectent pas le seuil de prévalence
```{r}
# Execute prevalence filter, using `prune_taxa()` function
keepTaxa = rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ps2 = prune_taxa(keepTaxa, ps)
```

# Agglomerate taxa
on sait que les communautés microbiennes sont souvent composées de taxons qui partagent des caractéristiques communes. On va donc chercher à mettre ensemble les taxons qui sont très proches les uns de autres.
Pour cela, l'aggregation taxonomique est pratique. Elle est facile, et on peut comparer les taxons grâce à des arbres simples à rangs. Pour le generer on va pouvoir utiliser phyloseq. La première chose qui sera faite sera d'agglomerer ensemble les échantillons du même genre. 
```{r}
# How many genera would be present after filtering?
length(get_taxa_unique(ps2, taxonomic.rank = "Genus"))
```
tax_glom est une fonction qui permet de rassembler les espèces ayant une taxonomie proche. On va donc mettre ces séquences là dans l'objet "ps3" qui va nous servir pour la construction de l'arbre.
```{r}
ps3 = tax_glom(ps2, "Genus", NArm = TRUE)
```
Tip_glom est une fonction analogue à tax_glom. Il nous permet de séparer les distances cophenetiques inférieures à une valeur h. La distance cophenetique est la distance entre deux objets dans l'arbre dont les branches comprennent deux objets réduits en une branche. On va donc créer un objet ps4 qui portera cette caractéristique. 
```{r}
h1 = 0.4
ps4 = tip_glom(ps2, h = h1)
```
ici phyloseq va comparer les datas originales par rapport à l'arbre obtenu après agglomeration taxonomiques et enfin à l'arbre après les agglomerations phylogéniques. Grâce à la fonction gridExtra, nous pourrons ainsi générer ces 3 objets en un.
```{r}
multiPlotTitleTextSize = 15
p2tree = plot_tree(ps2, method = "treeonly",
                   ladderize = "left",
                   title = "Before Agglomeration") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p3tree = plot_tree(ps3, method = "treeonly",
                   ladderize = "left", title = "By Genus") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p4tree = plot_tree(ps4, method = "treeonly",
                   ladderize = "left", title = "By Height") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
library (gridExtra)
gridExtra::grid.arrange
```
Sur la gauche nous retrouvons l'arbre original, au milieu l'arbre généré par agglomération taxonomique et à droit l'arbre généré par aggrégation phylogénique. On peut voir que les deux agglomérations nous permettent de clarifier les arbres. De plus, les arbres obtenus avec les deux types d'agglomération sont assez ressemblants.
```{r}
# group plots together
grid.arrange(nrow = 1, p2tree, p3tree, p4tree)
```


```{r}
pslog <-transform_sample_counts(ps, function(x) log(1 + x))
out.wuf.log <-ordinate(pslog, method = "PCoA", distance = "bray")
```

```{r}
evals <-out.wuf.log$values$Eigenvalues
plot_ordination(pslog, out.wuf.log, color = "Espece", shape="traitement") + labs(col = "Espece",shape="traitement")
```

```{r}
evals <-out.wuf.log$values$Eigenvalues
plot_ordination(pslog, out.wuf.log, color = "Espece", shape="time") + labs(col = "Espece",shape="time")
```

```{r}
# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))
ord.nmds.bray <- ordinate(ps.prop, method="NMDS", distance="bray")
```
```{r}
plot_ordination(ps.prop, ord.nmds.bray, color="Espece", shape="traitement", title="Bray NMDS")
```

```{r}
top20 <- names(sort(taxa_sums(ps), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Espece", fill="Family") + facet_wrap(~traitement, scales="free_x")
```
# Create table, number of features for each phyla
```{r}
table(tax_table(ps)[, "Phylum"], exclude = NULL)
```

```{r}
ps <- subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized"))
```
Ici nous allons mesurer la prévalence, qui sera dans le cadre de cette étude le nombre d'echantillons par taxon

```{r}
# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps),
               MARGIN = ifelse(taxa_are_rows(ps), yes = 1, no = 2),
               FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                    TotalAbundance = taxa_sums(ps),
                    tax_table(ps))
```
Cette commande nous permet d'evaluer la prévalence moyenne de chaque phylum (colonne1) et la prévalence totale (colonne2). Cela nous permet de confirmer les résultats du dessus et de pouvoir éliminer les phylums peu importants.
```{r}
plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```
Le code permettant de retirer ces taxons est ci-dessous.
```{r}
# Define phyla to filter
filterPhyla = c("WS2", "Deinococcota", "Nitrospirota","AncK6", "Dadabacteria ", "Crenarchaeota", "Gemmatimonadota", "SAR324 clade(Marine group B)", "Bacteroidota", "Margulisbacteria", "PAUC34f", "Myxococcota","unchara")
# Filter entries with unidentified Phylum.
ps1 = subset_taxa(ps, !Phylum %in% filterPhyla)
ps1
```
```{r}
# Subset to the remaining phyla
prevdf1 = subset(prevdf, Phylum %in% get_taxa_unique(ps1, "Phylum"))
ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps),color=Phylum)) +
  # Include a guess for parameter
  geom_hline(yintercept = 0.05, alpha = 0.5, linetype = 2) +  geom_point(size = 2, alpha = 0.7) +
  scale_x_log10() +  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) + theme(legend.position="none")
```
on va donc fixer un seuil de prévalence de 5%, c'est-à- dire que nous allons retirer toutes les valeurs de prévalence inferieures à 95%.
```{r}
# Define prevalence threshold as 5% of total samples
prevalenceThreshold = 0.05 * nsamples(ps)
prevalenceThreshold
```
C'est grâce à la fonction prune_taxa qu'on va pouvoir retirer les ASVs qui ne respectent pas le seuil de prévalence
```{r}
# Execute prevalence filter, using `prune_taxa()` function
keepTaxa = rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ps2 = prune_taxa(keepTaxa, ps)
```

```{r}
top20 <- names(sort(taxa_sums(ps2), decreasing=TRUE))[1:20]
ps.top20 <- transform_sample_counts(ps2, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.top20)
plot_bar(ps.top20, x="Espece", fill="Family") + facet_wrap(~traitement, scales="free_x")
```

#analyse en reseau
```{r}
net <- make_network(ps, max.dist=0.8)
sampledata <- data.frame(sample_data(ps))
V(net)$Eponge <- sampledata[names(V(net)), "Espece"]
V(net)$time <- sampledata[names(V(net)), "traitement"]
net_graph <- ggnetwork(net)
ggplot(net_graph, aes(x = x, y = y, xend = xend, yend = yend), layout = "fruchtermanreingold") +
  geom_edges(color = "darkgray") +
  geom_nodes(aes(color = Eponge, shape = time),  size = 3 ) +
  theme(axis.text = element_blank(), axis.title = element_blank(),
        legend.key.height = unit(0.5,"line")) +
  guides(col = guide_legend(override.aes = list(size = .5)))
```



